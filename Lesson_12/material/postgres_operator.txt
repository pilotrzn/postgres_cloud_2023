-- Postgres operator GKE - уменьшаем мощность машины и указываем только 1 зону 1 региона

--cluster-version "1.21.5-gke.1302" (21/11/18)
-- need to update in time
-- --cluster-version "1.22.8-gke.201" (22/06/06)
-- e2-medium, 30Gb
gcloud beta container --project "celtic-house-266612" clusters create "postgresoperator" --zone "us-central1-c" --no-enable-basic-auth --cluster-version "1.22.8-gke.201" --release-channel "regular" --machine-type "e2-medium" --image-type "COS_CONTAINERD" --disk-type "pd-standard" --disk-size "30" --metadata disable-legacy-endpoints=true --scopes "https://www.googleapis.com/auth/devstorage.read_only","https://www.googleapis.com/auth/logging.write","https://www.googleapis.com/auth/monitoring","https://www.googleapis.com/auth/servicecontrol","https://www.googleapis.com/auth/service.management.readonly","https://www.googleapis.com/auth/trace.append" --max-pods-per-node "110" --preemptible --num-nodes "3" --logging=SYSTEM,WORKLOAD --monitoring=SYSTEM --enable-ip-alias --network "projects/celtic-house-266612/global/networks/default" --subnetwork "projects/celtic-house-266612/regions/us-central1/subnetworks/default" --no-enable-intra-node-visibility --default-max-pods-per-node "110" --no-enable-master-authorized-networks --addons HorizontalPodAutoscaling,HttpLoadBalancing,GcePersistentDiskCsiDriver --enable-autoupgrade --enable-autorepair --max-surge-upgrade 1 --max-unavailable-upgrade 0 --enable-shielded-nodes --node-locations "us-central1-c"


NAME        LOCATION       MASTER_VERSION  MASTER_IP     MACHINE_TYPE  NODE_VERSION    NUM_NODES  STATUS
postgresha  us-central1-c  1.22.8-gke.201  35.222.45.52  e2-medium     1.22.8-gke.201  3          RUNNING

-- postgres operator
-- посмотрим существующие уже ресурсы
kubectl api-resources

git clone https://github.com/zalando/postgres-operator
cd postgres-operator
helm install postgres-operator ./charts/postgres-operator

-- убедимся, что postgres-operator стартовал:
kubectl --namespace=default get pods -l "app.kubernetes.io/name=postgres-operator"

-- посмотрим, что ресурс постгрес появился
kubectl api-resources | grep postgres

-- поставим UI к постгрес оператору
helm install postgres-operator-ui ./charts/postgres-operator-ui

-- To verify that postgres-operator has started, run:
kubectl --namespace=default get pods -l "app.kubernetes.io/name=postgres-operator-ui"

kubectl get all
kubectl port-forward svc/postgres-operator-ui 8081:80

-- http://localhost:8081/#new

-- создадим кластер через UI - на самом деле формирует ямл
name - minimal
instances - 2
галочка на pg_bouncer

-- посмотрим как развернулся
kubectl get all -A
kubectl get pods
kubectl get pv
kubectl get all -o wide

-- подробная информация о ноде
kubectl get node gke-postgresoperator-default-pool-e1414cd8-nd3m -o wide
gcloud compute disks list

-- сколько задействовано ресурсов
kubectl top node gke-postgresoperator-default-pool-e1414cd8-nd3m


-- Connect to the Postgres cluster via psql
-- You can create a port-forward on a database pod to connect to Postgres. 
-- See the user guide for instructions. With minikube it's also easy to retrieve the connections string 
-- from the K8s service that is pointing to the master pod:

-- IP GKE
export PGHOST="104.154.199.119"
export PGPORT="5432"

-- Retrieve the password FROM the K8s Secret that is created in your cluster. 
-- Non-encrypted connections are rejected by default, so set the SSL mode to require:

export PGPASSWORD=$(kubectl get secret postgres.acid-minimal.credentials.postgresql.acid.zalan.do -o 'jsonpath={.data.password}' | base64 -d)
echo $PGPASSWORD
export PGSSLMODE=require
-- извне доступ не получим, так как не указали создание Load Balancer
psql -U postgres

-- не сработало..
-- https://postgres-operator.readthedocs.io/en/latest/administrator/#load-balancers-and-allowed-ip-ranges

!!-- возможно селекторы неправильно у сервиса указаны
!! kubectl get pods -l spilo-role -L spilo-role

-- kubectl port-forward service/acid-minimal-pooler 5432:5432
-- pod/acid-minimal-pooler-6699dc666c-hfncx   0/1     Pending   0          2m3s

-- уменьшим требования
kubectl describe pod/acid-minimal-pooler-5b8c88f4d4-fdjr6
-- Warning  FailedScheduling  61s (x4 over 3m52s)  default-scheduler  0/3 nodes are available: 3 Insufficient cpu.
-- i для редактирования
-- :wq! - для выхода с сохранением
kubectl edit replicaset.apps/acid-minimal-pooler-5b8c88f4d4
kubectl get all
-- ничего не изменилось (



-- почему?



-- меняем деплоймент или репликасет???
kubectl get deployment
kubectl edit deployment.apps/acid-minimal-pooler
-- i для редактирования, что для выхода с сохранением?)

kubectl delete replicaset.apps/acid-minimal-pooler-5b8c88f4d4

kubectl port-forward service/acid-minimal-pooler 5432:5432

-- подключимся через psql
export PGPASSWORD=$(kubectl get secret postgres.acid-minimal.credentials.postgresql.acid.zalan.do -o 'jsonpath={.data.password}' | base64 -d)
export PGSSLMODE=disable

-- не получится, так как сертфикат выписан только на хост с Постгресом
psql -U postgres -h localhost 


-- зайдем на мастера
kubectl exec -it pod/acid-minimal-0 -- bash

psql -U postgres
exit

patronictl -c postgres.yml list


-- зайдем на пулер
echo $PGPASSWORD
kubectl exec -it pod/acid-minimal-pooler-bddf95b48-ftlzt -- psql -U postgres -h localhost sslmode=require -W

kubectl exec -it pod/acid-minimal-pooler-bddf95b48-ftlzt -- sh
df
pwd
netstat -a
pgbouncer -h
cat /etc/pgbouncer/pgbouncer.ini
ps
-- find / -name pgbouncer

-- убьем мастер
kubectl delete pod/acid-minimal-0


kubectl exec -it pod/acid-minimal-0 -- patronictl -c postgres.yml list

gcloud container clusters list
gcloud container clusters delete postgresoperator --zone us-central1-c

--посмотрим, что осталось от кластера
gcloud compute disks list